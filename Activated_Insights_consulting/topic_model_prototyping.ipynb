{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "#import cupy\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# import some text, in this case restuarant reviews\n",
    "\n",
    "yelp_business_datapath = '/home/matt_valley/PycharmProjects/insight_2020a_project/Resto_names/yelp_dataset/review.json'\n",
    "\n",
    "num_entries = 1000\n",
    "users = []\n",
    "with open(yelp_business_datapath) as fl:\n",
    "    for i, line in enumerate(fl):\n",
    "        users.append(json.loads(line))\n",
    "        if i+1 >= num_entries:\n",
    "            break\n",
    "df = pd.DataFrame(users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "\n",
    "model = 'en_core_web_sm' # for testing on laptop\n",
    "#model = 'en_core_web_lg'\n",
    "#model = 'en_vectors_web_lg' # many more words\n",
    "nlp = spacy.load(model)\n",
    "#sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "#nlp.add_pipe(sentencizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# rudimentary text chunking pipeline\n",
    "\n",
    "all_sentences = []\n",
    "all_sentence_entities = []\n",
    "all_tokens = []\n",
    "for r, review in enumerate(df.text):\n",
    "    doc = nlp(review)\n",
    "    tokens = [token.text for token in doc]\n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    sentence_entities = [ent.text for ent in doc.ents]\n",
    "    all_tokens.append(tokens)\n",
    "    all_sentences.append(sentences)\n",
    "    all_sentence_entities.append(sentence_entities)\n",
    "    \n",
    "df['tokens'] = all_tokens\n",
    "df['sentences'] = all_sentences\n",
    "df['entities'] = all_sentence_entities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "'''\n",
    "# do it another way, from https://gist.github.com/narulkargunjan/5319ed32d092d1fa7b52fec3a774e0e5\n",
    "columns=['text',\n",
    "           'log_probability',\n",
    "           'stop?',\n",
    "           'punctuation?',\n",
    "           'whitespace?',\n",
    "           'number?',\n",
    "           'out of vocab.?']\n",
    "token_df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "for r, review in enumerate(df.text):\n",
    "    doc = nlp(review)\n",
    "    token_attributes = [(token.orth_,\n",
    "                         token.prob,\n",
    "                         token.is_stop,\n",
    "                         token.is_punct,\n",
    "                         token.is_space,\n",
    "                         token.like_num,\n",
    "                         token.is_oov)\n",
    "                        for token in doc]\n",
    "    temp_df = pd.DataFrame(token_attributes, columns=columns)\n",
    "    token_df = token_df.append(temp_df)\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "done in 0.173s.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# data must be a list of strings\n",
    "data = [sent for sent in df.text]\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 10\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time.time()\n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "iteration: 1 of max_iter: 20\n",
      "iteration: 2 of max_iter: 20\n",
      "iteration: 3 of max_iter: 20\n",
      "iteration: 4 of max_iter: 20\n",
      "iteration: 5 of max_iter: 20\n",
      "iteration: 6 of max_iter: 20\n",
      "iteration: 7 of max_iter: 20\n",
      "iteration: 8 of max_iter: 20\n",
      "iteration: 9 of max_iter: 20\n",
      "iteration: 10 of max_iter: 20\n",
      "iteration: 11 of max_iter: 20\n",
      "iteration: 12 of max_iter: 20\n",
      "iteration: 13 of max_iter: 20\n",
      "iteration: 14 of max_iter: 20\n",
      "iteration: 15 of max_iter: 20\n",
      "iteration: 16 of max_iter: 20\n",
      "iteration: 17 of max_iter: 20\n",
      "iteration: 18 of max_iter: 20\n",
      "iteration: 19 of max_iter: 20\n",
      "iteration: 20 of max_iter: 20\n",
      "['00', '10', '100', '11', '12', '14', '15', '20', '25', '30', '40', '45', '50', '99', 'able', 'absolutely', 'ac', 'actually', 'add', 'added', 'additional', 'admit', 'affordable', 'afternoon', 'ago', 'ahead', 'air', 'airport', 'amazing', 'ambiance', 'answer', 'anymore', 'apparently', 'appetizer', 'appetizers', 'apple', 'appointment', 'appreciated', 'area', 'aren', 'arrive', 'arrived', 'asian', 'ask', 'asked', 'asking', 'ate', 'atmosphere', 'attention', 'attentive', 'attitude', 'authentic', 'available', 'average', 'avoid', 'away', 'awesome', 'awful', 'bacon', 'bad', 'bag', 'bakery', 'bar', 'barely', 'bartender', 'based', 'basic', 'basically', 'bbq', 'beans', 'beautiful', 'beef', 'beer', 'beers', 'believe', 'best', 'better', 'big', 'birthday', 'bit', 'bite', 'black', 'bland', 'boba', 'bonus', 'book', 'bottle', 'bought', 'bowl', 'box', 'boyfriend', 'bread', 'breakfast', 'bring', 'brisket', 'brought', 'brown', 'brunch', 'buffet', 'bunch', 'burger', 'burgers', 'burrito', 'business', 'busy', 'butter', 'buy', 'caesar', 'cafe', 'cake', 'california', 'called', 'came', 'car', 'caramel', 'card', 'care', 'carry', 'case', 'cash', 'casino', 'casual', 'center', 'certainly', 'chairs', 'change', 'changed', 'charge', 'charged', 'cheap', 'cheaper', 'check', 'checked', 'cheese', 'cheesy', 'chef', 'chicken', 'chinese', 'chips', 'chocolate', 'choice', 'choices', 'choose', 'chose', 'city', 'class', 'clean', 'clear', 'clearly', 'close', 'closed', 'club', 'cocktail', 'cocktails', 'coffee', 'cold', 'come', 'comes', 'comfortable', 'coming', 'company', 'complain', 'complaint', 'complaints', 'completely', 'considering', 'contact', 'continue', 'cooked', 'cookie', 'cool', 'corn', 'cost', 'couldn', 'counter', 'country', 'couple', 'course', 'covered', 'crab', 'crap', 'craving', 'crazy', 'cream', 'credit', 'crepe', 'crispy', 'crowded', 'crust', 'cup', 'cupcakes', 'curry', 'customer', 'customers', 'cut', 'cute', 'daily', 'dam', 'dark', 'date', 'daughter', 'day', 'days', 'deal', 'decent', 'decided', 'decor', 'definitely', 'delicious', 'delivery', 'desert', 'dessert', 'desserts', 'did', 'didn', 'die', 'different', 'difficult', 'dining', 'dinner', 'dip', 'dirty', 'disappointed', 'disappointing', 'discount', 'dish', 'dishes', 'doctor', 'does', 'doesn', 'dog', 'dogs', 'doing', 'dollars', 'don', 'door', 'downtown', 'dr', 'dress', 'dressing', 'drink', 'drinks', 'drive', 'driving', 'dry', 'duck', 'earlier', 'early', 'easy', 'eat', 'eating', 'egg', 'eggplant', 'eggs', 'email', 'employee', 'employees', 'enchiladas', 'end', 'ended', 'english', 'enjoy', 'enjoyed', 'enjoying', 'entire', 'entree', 'entrees', 'environment', 'equipment', 'especially', 'et', 'evening', 'exactly', 'excellent', 'excited', 'expect', 'expected', 'expecting', 'expensive', 'experience', 'explained', 'extra', 'extremely', 'fabulous', 'facility', 'fact', 'fair', 'fairly', 'fajitas', 'family', 'fan', 'fantastic', 'far', 'fast', 'favor', 'favorite', 'favorites', 'feel', 'feeling', 'feels', 'felt', 'filled', 'filling', 'finally', 'finding', 'fine', 'finish', 'finished', 'fish', 'fix', 'fixed', 'flavor', 'flavored', 'flavorful', 'flavors', 'floor', 'food', 'foot', 'forgot', 'forward', 'free', 'french', 'fresh', 'friday', 'fried', 'friend', 'friendly', 'friends', 'fries', 'frozen', 'fruit', 'fun', 'future', 'game', 'games', 'garlic', 'gas', 'gave', 'gem', 'gets', 'getting', 'giant', 'girl', 'girls', 'given', 'giving', 'glad', 'glass', 'glasses', 'gluten', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'grand', 'greasy', 'great', 'green', 'greeted', 'grill', 'grilled', 'grocery', 'group', 'groupon', 'guess', 'guy', 'guys', 'gym', 'hair', 'half', 'hand', 'hands', 'happen', 'happened', 'happy', 'hard', 'hate', 'having', 'head', 'healthy', 'hear', 'heard', 'heat', 'help', 'helped', 'helpful', 'high', 'highly', 'hit', 'hold', 'hole', 'home', 'homemade', 'honest', 'honestly', 'honey', 'hope', 'horrible', 'hostess', 'hot', 'hotel', 'hour', 'hours', 'house', 'hubby', 'huge', 'hungry', 'husband', 'ice', 'iced', 'idea', 'im', 'imagine', 'immediately', 'important', 'impressed', 'including', 'incredible', 'incredibly', 'indian', 'ingredients', 'inside', 'instead', 'insurance', 'interesting', 'interior', 'intimate', 'isn', 'issue', 'issues', 'italian', 'item', 'items', 'job', 'joint', 'juicy', 'just', 'kabob', 'kept', 'key', 'kid', 'kids', 'kind', 'kitchen', 'knew', 'know', 'knowledgeable', 'knows', 'la', 'lacking', 'ladies', 'lady', 'lamb', 'large', 'las', 'late', 'later', 'layout', 'leave', 'leaving', 'left', 'legs', 'lemon', 'let', 'lettuce', 'level', 'life', 'light', 'like', 'liked', 'limited', 'line', 'lines', 'list', 'literally', 'little', 'live', 'living', 'll', 'lobster', 'local', 'located', 'location', 'locations', 'lol', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lost', 'lot', 'lots', 'loud', 'love', 'loved', 'lovely', 'loves', 'low', 'lunch', 'mac', 'machine', 'machines', 'main', 'make', 'makes', 'making', 'mall', 'man', 'management', 'manager', 'mango', 'margarita', 'margaritas', 'market', 'mary', 'massage', 'matter', 'maybe', 'meal', 'meals', 'mean', 'means', 'meant', 'meat', 'medium', 'meet', 'member', 'mentioned', 'menu', 'met', 'mexican', 'middle', 'miles', 'milk', 'mind', 'mins', 'minute', 'minutes', 'miss', 'mistake', 'mom', 'money', 'month', 'months', 'morning', 'mouth', 'moved', 'multiple', 'mushroom', 'music', 'mustard', 'nail', 'nails', 'napkins', 'near', 'need', 'needed', 'needs', 'negative', 'neighborhood', 'new', 'nice', 'night', 'non', 'noodles', 'normal', 'normally', 'notch', 'note', 'noticed', 'obviously', 'offer', 'offered', 'office', 'oh', 'oil', 'ok', 'okay', 'old', 'olive', 'ones', 'onion', 'online', 'open', 'opened', 'opinion', 'option', 'options', 'order', 'ordered', 'ordering', 'orders', 'original', 'outdoor', 'outside', 'overall', 'overpriced', 'owned', 'owner', 'owners', 'packed', 'pad', 'paid', 'pain', 'parking', 'particular', 'parts', 'party', 'pass', 'past', 'pasta', 'patient', 'patio', 'pay', 'paying', 'people', 'peppers', 'perfect', 'perfectly', 'person', 'personal', 'personally', 'pho', 'phoenix', 'phone', 'piano', 'pick', 'picked', 'picture', 'pictures', 'pie', 'piece', 'pieces', 'pizza', 'place', 'placed', 'places', 'plan', 'plate', 'plates', 'play', 'pleasant', 'pleased', 'plenty', 'plus', 'point', 'pool', 'pork', 'portion', 'portions', 'positive', 'possible', 'post', 'pot', 'potato', 'potatoes', 'power', 'prefer', 'prepared', 'presentation', 'pretty', 'previous', 'price', 'priced', 'prices', 'pricey', 'pricing', 'prime', 'probably', 'problem', 'process', 'professional', 'provide', 'provided', 'pulled', 'purchase', 'purchased', 'quality', 'questions', 'quick', 'quickly', 'quite', 'ranch', 'range', 'rare', 'rate', 'ravioli', 'read', 'ready', 'real', 'really', 'reason', 'reasonable', 'receive', 'received', 'recently', 'recommend', 'recommended', 'red', 'refund', 'regular', 'relaxing', 'remember', 'repair', 'reservation', 'reservations', 'rest', 'restaurant', 'restaurants', 'return', 'returned', 'returning', 'review', 'reviews', 'rib', 'ribs', 'rice', 'rich', 'right', 'rings', 'road', 'roll', 'rolls', 'room', 'rooms', 'rude', 'run', 'running', 'safe', 'said', 'salad', 'salads', 'salmon', 'salon', 'salsa', 'salt', 'salty', 'sandwich', 'sandwiches', 'sashimi', 'sat', 'satisfied', 'saturday', 'sauce', 'sauces', 'sausage', 'save', 'saved', 'saw', 'say', 'saying', 'says', 'scheduled', 'scottsdale', 'screen', 'seafood', 'seasoned', 'seat', 'seated', 'seating', 'second', 'section', 'seeing', 'seen', 'selection', 'self', 'sell', 'sent', 'seriously', 'serve', 'served', 'server', 'servers', 'service', 'serving', 'set', 'share', 'shared', 'ship', 'shop', 'shopping', 'short', 'showed', 'shrimp', 'sides', 'sign', 'signed', 'simple', 'simply', 'single', 'sister', 'sit', 'sitting', 'situation', 'size', 'slice', 'slow', 'small', 'smaller', 'smell', 'smile', 'soda', 'soft', 'sold', 'solid', 'son', 'soon', 'sorry', 'soup', 'space', 'special', 'specials', 'spectacular', 'spend', 'spent', 'spicy', 'split', 'spot', 'staff', 'stand', 'standard', 'standing', 'star', 'stars', 'start', 'started', 'state', 'stay', 'stayed', 'steak', 'steaks', 'stick', 'stomach', 'stop', 'stopped', 'store', 'stores', 'story', 'strange', 'street', 'strip', 'stuck', 'stuff', 'style', 'sugar', 'suggested', 'summer', 'sunday', 'super', 'sure', 'surprise', 'surprised', 'sushi', 'sweet', 'table', 'tables', 'taco', 'tacos', 'taken', 'takes', 'taking', 'talk', 'talked', 'talking', 'taste', 'tasted', 'tasting', 'tasty', 'tea', 'team', 'tell', 'telling', 'tells', 'tender', 'terrible', 'thai', 'thank', 'thanks', 'thing', 'things', 'think', 'thinking', 'thought', 'time', 'times', 'tip', 'toast', 'today', 'told', 'tomato', 'tons', 'took', 'topped', 'toppings', 'toronto', 'total', 'totally', 'touch', 'town', 'traditional', 'travel', 'treat', 'treated', 'tried', 'trip', 'truly', 'trust', 'try', 'trying', 'tuna', 'turkey', 'turned', 'tv', 'twice', 'type', 'typical', 'understand', 'unfortunately', 'unique', 'unless', 'use', 'used', 'using', 'usual', 'usually', 'valley', 'value', 'variety', 've', 'vegan', 'vegas', 'vegetables', 'vegetarian', 'veggies', 'venetian', 'venue', 'vibe', 'view', 'visit', 'visited', 'visiting', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'walk', 'walked', 'walking', 'wall', 'want', 'wanted', 'warm', 'wasn', 'watch', 'watched', 'watching', 'water', 'way', 'wedding', 'week', 'weekend', 'weeks', 'weird', 'welcoming', 'went', 'weren', 'white', 'wide', 'wife', 'willing', 'window', 'wine', 'wings', 'wish', 'woman', 'won', 'wonderful', 'work', 'worked', 'working', 'world', 'worse', 'worst', 'worth', 'wouldn', 'wow', 'write', 'wrong', 'yeah', 'year', 'years', 'yelp', 'yes', 'yum', 'yummy']\n",
      "['00', '10', '100', '11', '12', '14', '15', '20', '25', '30', '40', '45', '50', '99', 'able', 'absolutely', 'ac', 'actually', 'add', 'added', 'additional', 'admit', 'affordable', 'afternoon', 'ago', 'ahead', 'air', 'airport', 'amazing', 'ambiance', 'answer', 'anymore', 'apparently', 'appetizer', 'appetizers', 'apple', 'appointment', 'appreciated', 'area', 'aren', 'arrive', 'arrived', 'asian', 'ask', 'asked', 'asking', 'ate', 'atmosphere', 'attention', 'attentive', 'attitude', 'authentic', 'available', 'average', 'avoid', 'away', 'awesome', 'awful', 'bacon', 'bad', 'bag', 'bakery', 'bar', 'barely', 'bartender', 'based', 'basic', 'basically', 'bbq', 'beans', 'beautiful', 'beef', 'beer', 'beers', 'believe', 'best', 'better', 'big', 'birthday', 'bit', 'bite', 'black', 'bland', 'boba', 'bonus', 'book', 'bottle', 'bought', 'bowl', 'box', 'boyfriend', 'bread', 'breakfast', 'bring', 'brisket', 'brought', 'brown', 'brunch', 'buffet', 'bunch', 'burger', 'burgers', 'burrito', 'business', 'busy', 'butter', 'buy', 'caesar', 'cafe', 'cake', 'california', 'called', 'came', 'car', 'caramel', 'card', 'care', 'carry', 'case', 'cash', 'casino', 'casual', 'center', 'certainly', 'chairs', 'change', 'changed', 'charge', 'charged', 'cheap', 'cheaper', 'check', 'checked', 'cheese', 'cheesy', 'chef', 'chicken', 'chinese', 'chips', 'chocolate', 'choice', 'choices', 'choose', 'chose', 'city', 'class', 'clean', 'clear', 'clearly', 'close', 'closed', 'club', 'cocktail', 'cocktails', 'coffee', 'cold', 'come', 'comes', 'comfortable', 'coming', 'company', 'complain', 'complaint', 'complaints', 'completely', 'considering', 'contact', 'continue', 'cooked', 'cookie', 'cool', 'corn', 'cost', 'couldn', 'counter', 'country', 'couple', 'course', 'covered', 'crab', 'crap', 'craving', 'crazy', 'cream', 'credit', 'crepe', 'crispy', 'crowded', 'crust', 'cup', 'cupcakes', 'curry', 'customer', 'customers', 'cut', 'cute', 'daily', 'dam', 'dark', 'date', 'daughter', 'day', 'days', 'deal', 'decent', 'decided', 'decor', 'definitely', 'delicious', 'delivery', 'desert', 'dessert', 'desserts', 'did', 'didn', 'die', 'different', 'difficult', 'dining', 'dinner', 'dip', 'dirty', 'disappointed', 'disappointing', 'discount', 'dish', 'dishes', 'doctor', 'does', 'doesn', 'dog', 'dogs', 'doing', 'dollars', 'don', 'door', 'downtown', 'dr', 'dress', 'dressing', 'drink', 'drinks', 'drive', 'driving', 'dry', 'duck', 'earlier', 'early', 'easy', 'eat', 'eating', 'egg', 'eggplant', 'eggs', 'email', 'employee', 'employees', 'enchiladas', 'end', 'ended', 'english', 'enjoy', 'enjoyed', 'enjoying', 'entire', 'entree', 'entrees', 'environment', 'equipment', 'especially', 'et', 'evening', 'exactly', 'excellent', 'excited', 'expect', 'expected', 'expecting', 'expensive', 'experience', 'explained', 'extra', 'extremely', 'fabulous', 'facility', 'fact', 'fair', 'fairly', 'fajitas', 'family', 'fan', 'fantastic', 'far', 'fast', 'favor', 'favorite', 'favorites', 'feel', 'feeling', 'feels', 'felt', 'filled', 'filling', 'finally', 'finding', 'fine', 'finish', 'finished', 'fish', 'fix', 'fixed', 'flavor', 'flavored', 'flavorful', 'flavors', 'floor', 'food', 'foot', 'forgot', 'forward', 'free', 'french', 'fresh', 'friday', 'fried', 'friend', 'friendly', 'friends', 'fries', 'frozen', 'fruit', 'fun', 'future', 'game', 'games', 'garlic', 'gas', 'gave', 'gem', 'gets', 'getting', 'giant', 'girl', 'girls', 'given', 'giving', 'glad', 'glass', 'glasses', 'gluten', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'grand', 'greasy', 'great', 'green', 'greeted', 'grill', 'grilled', 'grocery', 'group', 'groupon', 'guess', 'guy', 'guys', 'gym', 'hair', 'half', 'hand', 'hands', 'happen', 'happened', 'happy', 'hard', 'hate', 'having', 'head', 'healthy', 'hear', 'heard', 'heat', 'help', 'helped', 'helpful', 'high', 'highly', 'hit', 'hold', 'hole', 'home', 'homemade', 'honest', 'honestly', 'honey', 'hope', 'horrible', 'hostess', 'hot', 'hotel', 'hour', 'hours', 'house', 'hubby', 'huge', 'hungry', 'husband', 'ice', 'iced', 'idea', 'im', 'imagine', 'immediately', 'important', 'impressed', 'including', 'incredible', 'incredibly', 'indian', 'ingredients', 'inside', 'instead', 'insurance', 'interesting', 'interior', 'intimate', 'isn', 'issue', 'issues', 'italian', 'item', 'items', 'job', 'joint', 'juicy', 'just', 'kabob', 'kept', 'key', 'kid', 'kids', 'kind', 'kitchen', 'knew', 'know', 'knowledgeable', 'knows', 'la', 'lacking', 'ladies', 'lady', 'lamb', 'large', 'las', 'late', 'later', 'layout', 'leave', 'leaving', 'left', 'legs', 'lemon', 'let', 'lettuce', 'level', 'life', 'light', 'like', 'liked', 'limited', 'line', 'lines', 'list', 'literally', 'little', 'live', 'living', 'll', 'lobster', 'local', 'located', 'location', 'locations', 'lol', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lost', 'lot', 'lots', 'loud', 'love', 'loved', 'lovely', 'loves', 'low', 'lunch', 'mac', 'machine', 'machines', 'main', 'make', 'makes', 'making', 'mall', 'man', 'management', 'manager', 'mango', 'margarita', 'margaritas', 'market', 'mary', 'massage', 'matter', 'maybe', 'meal', 'meals', 'mean', 'means', 'meant', 'meat', 'medium', 'meet', 'member', 'mentioned', 'menu', 'met', 'mexican', 'middle', 'miles', 'milk', 'mind', 'mins', 'minute', 'minutes', 'miss', 'mistake', 'mom', 'money', 'month', 'months', 'morning', 'mouth', 'moved', 'multiple', 'mushroom', 'music', 'mustard', 'nail', 'nails', 'napkins', 'near', 'need', 'needed', 'needs', 'negative', 'neighborhood', 'new', 'nice', 'night', 'non', 'noodles', 'normal', 'normally', 'notch', 'note', 'noticed', 'obviously', 'offer', 'offered', 'office', 'oh', 'oil', 'ok', 'okay', 'old', 'olive', 'ones', 'onion', 'online', 'open', 'opened', 'opinion', 'option', 'options', 'order', 'ordered', 'ordering', 'orders', 'original', 'outdoor', 'outside', 'overall', 'overpriced', 'owned', 'owner', 'owners', 'packed', 'pad', 'paid', 'pain', 'parking', 'particular', 'parts', 'party', 'pass', 'past', 'pasta', 'patient', 'patio', 'pay', 'paying', 'people', 'peppers', 'perfect', 'perfectly', 'person', 'personal', 'personally', 'pho', 'phoenix', 'phone', 'piano', 'pick', 'picked', 'picture', 'pictures', 'pie', 'piece', 'pieces', 'pizza', 'place', 'placed', 'places', 'plan', 'plate', 'plates', 'play', 'pleasant', 'pleased', 'plenty', 'plus', 'point', 'pool', 'pork', 'portion', 'portions', 'positive', 'possible', 'post', 'pot', 'potato', 'potatoes', 'power', 'prefer', 'prepared', 'presentation', 'pretty', 'previous', 'price', 'priced', 'prices', 'pricey', 'pricing', 'prime', 'probably', 'problem', 'process', 'professional', 'provide', 'provided', 'pulled', 'purchase', 'purchased', 'quality', 'questions', 'quick', 'quickly', 'quite', 'ranch', 'range', 'rare', 'rate', 'ravioli', 'read', 'ready', 'real', 'really', 'reason', 'reasonable', 'receive', 'received', 'recently', 'recommend', 'recommended', 'red', 'refund', 'regular', 'relaxing', 'remember', 'repair', 'reservation', 'reservations', 'rest', 'restaurant', 'restaurants', 'return', 'returned', 'returning', 'review', 'reviews', 'rib', 'ribs', 'rice', 'rich', 'right', 'rings', 'road', 'roll', 'rolls', 'room', 'rooms', 'rude', 'run', 'running', 'safe', 'said', 'salad', 'salads', 'salmon', 'salon', 'salsa', 'salt', 'salty', 'sandwich', 'sandwiches', 'sashimi', 'sat', 'satisfied', 'saturday', 'sauce', 'sauces', 'sausage', 'save', 'saved', 'saw', 'say', 'saying', 'says', 'scheduled', 'scottsdale', 'screen', 'seafood', 'seasoned', 'seat', 'seated', 'seating', 'second', 'section', 'seeing', 'seen', 'selection', 'self', 'sell', 'sent', 'seriously', 'serve', 'served', 'server', 'servers', 'service', 'serving', 'set', 'share', 'shared', 'ship', 'shop', 'shopping', 'short', 'showed', 'shrimp', 'sides', 'sign', 'signed', 'simple', 'simply', 'single', 'sister', 'sit', 'sitting', 'situation', 'size', 'slice', 'slow', 'small', 'smaller', 'smell', 'smile', 'soda', 'soft', 'sold', 'solid', 'son', 'soon', 'sorry', 'soup', 'space', 'special', 'specials', 'spectacular', 'spend', 'spent', 'spicy', 'split', 'spot', 'staff', 'stand', 'standard', 'standing', 'star', 'stars', 'start', 'started', 'state', 'stay', 'stayed', 'steak', 'steaks', 'stick', 'stomach', 'stop', 'stopped', 'store', 'stores', 'story', 'strange', 'street', 'strip', 'stuck', 'stuff', 'style', 'sugar', 'suggested', 'summer', 'sunday', 'super', 'sure', 'surprise', 'surprised', 'sushi', 'sweet', 'table', 'tables', 'taco', 'tacos', 'taken', 'takes', 'taking', 'talk', 'talked', 'talking', 'taste', 'tasted', 'tasting', 'tasty', 'tea', 'team', 'tell', 'telling', 'tells', 'tender', 'terrible', 'thai', 'thank', 'thanks', 'thing', 'things', 'think', 'thinking', 'thought', 'time', 'times', 'tip', 'toast', 'today', 'told', 'tomato', 'tons', 'took', 'topped', 'toppings', 'toronto', 'total', 'totally', 'touch', 'town', 'traditional', 'travel', 'treat', 'treated', 'tried', 'trip', 'truly', 'trust', 'try', 'trying', 'tuna', 'turkey', 'turned', 'tv', 'twice', 'type', 'typical', 'understand', 'unfortunately', 'unique', 'unless', 'use', 'used', 'using', 'usual', 'usually', 'valley', 'value', 'variety', 've', 'vegan', 'vegas', 'vegetables', 'vegetarian', 'veggies', 'venetian', 'venue', 'vibe', 'view', 'visit', 'visited', 'visiting', 'wait', 'waited', 'waiter', 'waiting', 'waitress', 'walk', 'walked', 'walking', 'wall', 'want', 'wanted', 'warm', 'wasn', 'watch', 'watched', 'watching', 'water', 'way', 'wedding', 'week', 'weekend', 'weeks', 'weird', 'welcoming', 'went', 'weren', 'white', 'wide', 'wife', 'willing', 'window', 'wine', 'wings', 'wish', 'woman', 'won', 'wonderful', 'work', 'worked', 'working', 'world', 'worse', 'worst', 'worth', 'wouldn', 'wow', 'write', 'wrong', 'yeah', 'year', 'years', 'yelp', 'yes', 'yum', 'yummy']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#Do LDA\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components,random_state=0, verbose=1, max_iter=20)\n",
    "lda.fit(tfidf)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "print(tf_feature_names)\n",
    "print(tfidf_feature_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Topic #0: facility la clear tried et joint boba cut month rate\n",
      "Topic #1: valley indian healthy tv sauces games entrees craving especially location\n",
      "Topic #2: salon crepe nail nails kabob class english chinese area bonus\n",
      "Topic #3: piano unless multiple near choices pizza soup open lots house\n",
      "Topic #4: venue attitude overpriced ship asian seat basically steak fried wow\n",
      "Topic #5: great miss ask flavored online cheap prepared service needs car\n",
      "Topic #6: sushi crab seafood buffet brunch breakfast crispy rolls pricing awful\n",
      "Topic #7: cake tea ice cream used chocolate bowl apparently cold looks\n",
      "Topic #8: food good place great really chicken like just service delicious\n",
      "Topic #9: team daily stick curry spend range honey soda 14 spectacular\n",
      "Topic #10: email pizza cheesy staff napkins burger awesome layout pricing serve\n",
      "Topic #11: hair massage goes relaxing foot picture filled matter boyfriend particular\n",
      "Topic #12: sugar eggplant love cocktails class bartender fabulous desert morning pie\n",
      "Topic #13: thai trip pad machines sister driving looks play fun walk\n",
      "Topic #14: great service place staff time good love awesome work just\n",
      "Topic #15: pizza breakfast today coffee dr look professional phone office enjoyed\n",
      "Topic #16: hands et lobster pizza grill comfortable wanted kids stuck airport\n",
      "Topic #17: enchiladas affordable et cute area incredible unique online taco thank\n",
      "Topic #18: safe entrees barely country tasting seeing plates forward serve waiter\n",
      "Topic #19: authentic middle pot im hot choice sitting ambiance fajitas frozen\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print_top_words(lda, tfidf_feature_names, n_top_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(2745, 2)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "                                                  Message  Target\nYelp 0                                 Crust is not good.       0\n     1          Not tasty and the texture was just nasty.       0\n     2  Stopped by during the late May bank holiday of...       1\n     3  The selection on the menu was great and so wer...       1\n     4     Now I am getting angry and I want my damn pho.       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>Message</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Yelp</th>\n      <th>0</th>\n      <td>Crust is not good.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Not tasty and the texture was just nasty.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Stopped by during the late May bank holiday of...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The selection on the menu was great and so wer...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Now I am getting angry and I want my damn pho.</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 47
    }
   ],
   "source": [
    "# SENTIMENT ANALYSIS\n",
    "\n",
    "# see: From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015 \n",
    "# http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "\n",
    "\n",
    "# Load our dataset\n",
    "df_yelp = pd.read_table('~/PycharmProjects/insight_2020a_project/Activated_Insights_consulting/sentiment_labelled_sentences/yelp_labelled.txt')\n",
    "df_imdb = pd.read_table('~/PycharmProjects/insight_2020a_project/Activated_Insights_consulting/sentiment_labelled_sentences/imdb_labelled.txt')\n",
    "df_amz = pd.read_table('~/PycharmProjects/insight_2020a_project/Activated_Insights_consulting/sentiment_labelled_sentences/amazon_cells_labelled.txt')\n",
    "\n",
    "frames = [df_yelp,df_imdb,df_amz]\n",
    "\n",
    "for colname in frames:\n",
    "    colname.columns = [\"Message\",\"Target\"]\n",
    "    \n",
    "# Assign a Key to Make it Easier\n",
    "keys = ['Yelp','IMDB','Amazon']\n",
    "sentiment_df = pd.concat(frames,keys=keys)\n",
    "\n",
    "print(sentiment_df.shape)\n",
    "sentiment_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "sentiment_model = spacy.load(model)\n",
    "stopwords = list(STOP_WORDS)\n",
    "punctuations = string.punctuation\n",
    "parser = English()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# from https://github.com/Jcharis/Natural-Language-Processing-Tutorials/blob/master/Text%20Classification%20With%20Machine%20Learning,SpaCy,Sklearn(Sentiment%20Analysis)/Text%20Classification%20&%20Sentiment%20Analysis%20with%20SpaCy,Sklearn.ipynb\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n",
    "    mytokens = [word for word in mytokens if word not in stopwords and word not in punctuations]\n",
    "    return mytokens\n",
    "\n",
    "tokens = [spacy_tokenizer(s[0].text) for s in df.sentences]\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "#Custom transformer using spaCy \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text \n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)) \n",
    "classifier = LinearSVC()\n",
    "\n",
    "\n",
    "# Features and Labels\n",
    "X = sentiment_df['Message']\n",
    "ylabels = sentiment_df['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the  pipeline to clean, tokenize, vectorize, and classify \n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('classifier', classifier)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('cleaner', <__main__.predictors object at 0x7f83af95d790>),\n                ('vectorizer',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<function spacy_tokenizer at 0x7f83a5b603b0>,\n                                 vocabulary=None)),\n                ('classifier',\n                 LinearSVC(C=1.0, class_weight=None, dual=True,\n                           fit_intercept=True, intercept_scaling=1,\n                           loss='squared_hinge', max_iter=1000,\n                           multi_class='ovr', penalty='l2', random_state=None,\n                           tol=0.0001, verbose=0))],\n         verbose=False)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 62
    }
   ],
   "source": [
    "# Fit our data\n",
    "pipe.fit(X_train,y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# Predicting with a test dataset\n",
    "sample_prediction = pipe.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Test accuracy:  0.8069216757741348\n",
      "Train accuracy:  0.9895264116575592\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(\"Test accuracy: \",pipe.score(X_test,y_test))\n",
    "print(\"Train accuracy: \",pipe.score(X_train,y_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# VECTORIZE SENTENCES AND SENT FRAGMENTS\n",
    "\n",
    "model = 'en_core_web_sm' # for testing on laptop\n",
    "#model = 'en_core_web_lg'\n",
    "#model = 'en_vectors_web_lg' # many more words\n",
    "nlp = spacy.load(model)\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def clean_doc(doc, pos_filter=['NOUN', 'VERB', 'ADV']):\n",
    "    # take input document and vectorize in context, and clean\n",
    "    # for topic model return nouns and verbs\n",
    "    # for sentiment return  adverbs    \n",
    "    token_indices = [i for i,t in enumerate(doc) if t.pos_ in pos_filter]\n",
    "    tokens = [t for i,t in enumerate(doc) if t.pos_ in pos_filter]\n",
    "    token_vectors = [t.vector for i,t in enumerate(doc) if t.pos_ in pos_filter]\n",
    "    return tokens, token_vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d7ef9a8a340a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spacy/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Sentencizer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtoken.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.is_sent_start.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E043] Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state."
     ],
     "ename": "ValueError",
     "evalue": "[E043] Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state.",
     "output_type": "error"
    }
   ],
   "source": [
    "for r, review in enumerate(df.text[:2]):\n",
    "    doc = nlp(review)\n",
    "    doc = clean_doc(doc) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Total ADJ\n",
      "bill NOUN\n",
      "for ADP\n",
      "this DET\n",
      "horrible ADJ\n",
      "service NOUN\n",
      "? PUNCT\n",
      "Over ADP\n",
      "$ SYM\n",
      "8Gs PROPN\n",
      ". PUNCT\n",
      "These DET\n",
      "crooks NOUN\n",
      "actually ADV\n",
      "had AUX\n",
      "the DET\n",
      "nerve NOUN\n",
      "to PART\n",
      "charge VERB\n",
      "us PRON\n",
      "$ SYM\n",
      "69 NUM\n",
      "for ADP\n",
      "3 NUM\n",
      "pills NOUN\n",
      ". PUNCT\n",
      "I PRON\n",
      "checked VERB\n",
      "online ADV\n",
      "the DET\n",
      "pills NOUN\n",
      "can VERB\n",
      "be AUX\n",
      "had VERB\n",
      "for ADP\n",
      "19 NUM\n",
      "cents NOUN\n",
      "EACH NOUN\n",
      "! PUNCT\n",
      "Avoid VERB\n",
      "Hospital NOUN\n",
      "ERs VERB\n",
      "at ADP\n",
      "all DET\n",
      "costs NOUN\n",
      ". PUNCT\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "doc = nlp(df.text[0])\n",
    "for t,tok in enumerate(doc):\n",
    "    print(tok.text, tok.pos_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for s, sent in enumerate(df.sentences):\n",
    "    doc = nlp(sent[0].text)\n",
    "    vectors.append(doc.vector)\n",
    "df['sentence_vector'] = vectors\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "sent_vec_mat = [vec for vec in df.sentence_vector.values]\n",
    "sent_vec_mat = np.array(sent_vec_mat)\n",
    "print(sent_vec_mat.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f056d84",
   "language": "python",
   "display_name": "PyCharm (insight_2020a_project)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}